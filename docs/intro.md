# Speech Enhancement on iOS: Local Denoising Solutions

Enhancing speech audio from low-quality sources on iOS can be tackled with both classic DSP algorithms and state-of-the-art ML models. Below we explore on-device speech enhancement solutions – from ready-made SDKs to open-source models – with notes on integration, performance, Core ML/Metal support, and suitability for real-time use. The focus is on speech (voice clarity, noise reduction, dereverberation), not general audio upscaling.

## Approaches to On-Device Speech Enhancement

**Traditional DSP Algorithms vs. ML Models**: Traditional methods (e.g. spectral subtraction, Wiener filters) run fast on CPU and are easier to implement but may offer limited improvement. Modern deep learning models can significantly improve speech quality by learning complex noise and reverberation patterns. Many ML models now match or exceed classic methods in quality, while being optimized for real-time, even on mobile hardware. A hybrid approach is also common: use DSP for simple parts and a neural network for the hard parts (this was the `RNNoise` design).

**Key requirements** for mobile use include: low latency (processing within a few milliseconds per audio frame), lightweight runtime (running on device without heavy CPU/GPU use), and quality comparable to cloud or desktop solutions. We also consider dereverberation (removing room echo) as part of speech enhancement, since residual reverb can muddle speech. Some advanced solutions explicitly handle dereverberation and even removal of competing speakers.

Below, we categorize available solutions:
- **Ready-to-Use SDKs/Frameworks**: High-level libraries with noise suppression for iOS.
- **Open-Source ML Models**: State-of-the-art research models that can be integrated (often via Core ML or TFLite).
- **Traditional Algorithms/Libraries**: DSP-based methods proven on mobile/embedded platforms.

## Ready-to-Use SDKs for iOS

### Krisp Noise Cancellation SDK

`Krisp` is a well-known commercial solution for AI noise cancellation. Their SDK (available for iOS, Android, desktop, and web) provides real-time voice noise removal and even voice separation, powered by deep neural networks. Integration is straightforward (e.g. via Swift Package or framework), and it runs fully on-device – no cloud needed. `Krisp`'s models are highly optimized for real-time; they boast usage in millions of devices and popular apps (video conferencing, streaming, call centers).
- **Performance**: Designed for streaming audio, filtering noise with minimal latency. It filters background sounds in real time (inbound and outbound audio). (Microsoft Teams and Zoom offer similar AI noise suppression, likely with heavy models on desktop, but `Krisp` manages this on mobile.)
- **Integration Ease**: High. The SDK abstracts the complexity – you initialize the noise canceller and feed it audio frames. For example, Stream's Video SDK simply enables a `NoiseCancellationFilter` that internally uses `Krisp` tech.
- **Core ML/Metal**: Not applicable (`Krisp` provides its own binaries; internal acceleration details are not exposed).
- **License**: Proprietary commercial license. `Krisp` requires contacting them for SDK access (free limited trials available).

**Pros**: Best-in-class noise reduction quality (trained on diverse noise). Little dev effort. Real-time proven.
**Cons**: Not open-source; licensing costs. Less flexibility (as a black-box). Focused on noise; not sure if dereverb is explicitly addressed.

### Picovoice Koala SDK

Picovoice `Koala` is an on-device noise suppression engine, also powered by deep learning. It's provided as an offline SDK for multiple platforms (iOS included). `Koala` is advertised to "remove noise from speech audio in real-time" on-device, with no cloud required. Its design is informed by Mozilla's `RNNoise`, but with Picovoice's own model and optimizations.
- **Performance**: `Koala` is very lightweight – comparable to `RNNoise` – and suitable for streaming audio on mobile. In benchmarks, `Koala` and `RNNoise` both run well below real-time on a single CPU thread (RTF < 1.0), meaning they can process audio faster than it's captured. `Koala`'s real-time factor and low resource use make it viable even for continuous use on a smartphone.
- **Integration Ease**: High. Picovoice provides Swift/C APIs (CocoaPod or SwiftPM). You obtain an `accessKey` then create a `Koala` instance and call `process()` on each 20ms frame of audio. A reference iOS demo is available. The SDK handles all DSP/ML internally.
- **Core ML/Metal**: Not needed (custom engine). `Koala` runs on CPU (or possibly uses NEON). It's packaged as a library (under Apache-2.0 license for the wrapper code). No Core ML model is provided since it's pre-optimized native code.
- **License**: Freemium/Commercial. The SDK requires a Picovoice license key (free tier available for personal/dev use). The code bindings are open-source (Apache-2.0) but the model is precompiled.

**Pros**: Very small footprint, real-time on iOS even with no hardware acceleration. Easy to integrate. Good speech intelligibility improvement (Picovoice reports better STOI scores than `RNNoise` in their tests).
**Cons**: Primarily targets background noise removal (does not explicitly perform dereverberation). Minor quality gap compared to larger models, but excellent given its speed. Requires signup for key.

### Apple Built-in Voice Processing

Apple's iOS offers built-in voice processing (used in FaceTime, Phone, etc.), which includes noise suppression, acoustic echo cancellation, and voice gating. Developers can access some of this via `AVAudioSessionModeVoiceChat` or `Voice Processing I/O` audio unit. For live audio capture, setting the audio session to voice-chat mode enables Apple's tuned noise reduction and echo canceler (optimized for device microphones). This is zero-integration cost – just an OS feature.
- **Performance**: Real-time and optimized on Apple hardware (possibly using the DSP in the audio codec or neural engine in newer iPhones for "Voice Isolation"). It works with low latency and minimal CPU usage (Apple doesn't publish specifics, but it's built for phone call performance).
- **Integration Ease**: Moderate. For processing an MP4 file's audio, one cannot directly call the built-in filter (it's tied to input pipeline). A workaround is routing file playback through an `AVAudioEngine` with the `VoiceProcessingIO` unit, capturing the output. This is somewhat complex and not officially documented for file use. (In real-time VoIP apps, it's straightforward – just enable the mode.)
- **Core ML/Metal**: N/A (proprietary Apple implementation).
- **License**: Included in iOS (no additional license needed).

**Pros**: Uses Apple's proprietary algorithms, likely optimized for common noise and tuned per device (multi-mic noise cancelation, etc.). No model integration needed.
**Cons**: Limited control or insight into what it does. Not explicitly designed for offline file processing (more for live input). Doesn't allow custom tuning – it's "on/off." Also, it's primarily noise cancel + echo cancel; it may not aggressively enhance speech beyond noise removal.

*Aside*: If targeting future live WebRTC streams, leveraging WebRTC's own processing or Apple's voice-processing might be an option for baseline noise reduction.

## Open-Source ML Models for Speech Enhancement

For a deeper customization (and possibly higher speech quality), several open-source ML models can be ported to iOS. These require integrating a model (via Core ML, TensorFlow Lite, or a C++ runtime) but offer flexibility and cutting-edge performance. Below are notable models:

### Mozilla RNNoise

`RNNoise` is a pioneering small-footprint RNN model for noise suppression, by Jean-Marc Valin (Xiph/Mozilla). It combines classic DSP (band-wise filtering) with a recurrent neural network that learns to suppress noise. `RNNoise` is lightweight – only ~60 kB of neural weights – and runs in real-time even on low-power hardware (7× faster than real-time on a Raspberry Pi 3). It was one of the first ML-based suppressors and is still widely used (e.g. in OBS Studio, Discord, etc.).
- **Quality**: `RNNoise` is effective against constant or environmental noise (e.g. fan, crowd, engine) and generally improves speech SNR. It performs voice activity detection internally and applies adaptive filtering per frequency band. However, it may introduce slight artifacts ("musical noise" or faint warbling) under very heavy noise or when misidentifying speech. Newer models outperform it in extremely noisy or reverberant cases, but it remains impressive for its size.
- **Integration**: Medium effort. `RNNoise` is in C (with no external dependencies), under a permissive BSD license. You can compile it as an iOS static library and feed audio frames (frame length 20 ms at 48 kHz by default). The library processes 16-bit PCM samples, outputting a denoised signal in the same format. No Core ML conversion is needed (it wouldn't benefit much given the model's small size; CPU is fine).
- **Performance**: Real-time easily. On an x86 CPU it runs 60× faster than real-time without special optimization. On an ARM iPhone CPU core, it uses a tiny fraction of CPU (consider that a Raspberry Pi 3 could run it 7× real-time in 2017 – an iPhone's A-series chip is far more powerful). It processes audio in 10 ms increments internally, so latency is very low.
- **Core ML/Metal**: Not applicable (would provide negligible benefit). The provided C implementation is already efficient, including vectorization hooks.
- **License**: BSD-3-Clause open source.

**Pros**: Open-source and free. Very small memory and CPU footprint – ideal for real-time. Proven in many applications. Easy to integrate at the C API level.
**Cons**: Quality is good but not state-of-the-art in 2025 – struggles with heavy reverberation or very complex noise. No built-in dereverb. It's a single-channel suppressor (cannot handle multi-mic or spatial noise on its own).

*(Note: `RNNoise` improvements like `PercepNet` have been proposed, using a similar strategy with perceptual features. Also, `RNNoise` is used as a component in other projects – e.g., Picovoice `Koala` was benchmarked against it.)*

### DTLN (Dual-Transform Layer Network)

`DTLN` is a real-time speech denoising model introduced in an academic paper (Interspeech 2020). It uses a two-stage neural network: one stage operates in the frequency domain (on magnitude spectra) and the second stage operates on the time-domain signal, each stage using LSTM layers. This allows it to reduce noise and refine the result, addressing both magnitude and phase to some extent. Despite being a deep model, `DTLN` is designed for real-time efficiency and has <1 million parameters (~4 MB).
- **Quality**: `DTLN` was a top contender in Microsoft's DNS Challenge 2020. It can achieve quality close to heavier models like U-Net or GAN-based enhancers, while maintaining much lower complexity. In one evaluation, `DTLN` achieved ~90% of the quality improvement of a state-of-the-art model, but with far less compute. One caveat: the original `DTLN` struggled with dereverberation (it could introduce a slight metallic "rustle" for reverberant inputs), but this can be mitigated by fine-tuning or additional processing. Newer versions or tweaks (as done by VK.com's team) address that for clearer output.
- **Integration**: Medium-High effort. An open-source implementation of `DTLN` is available in TensorFlow 2 (with pre-trained models). Pre-trained weights are provided in TensorFlow SavedModel, TFLite, and ONNX formats. This means you can convert the ONNX or TFLite model to Core ML using Apple's `coremltools`, or use TFLite on-device. Converting to a Core ML `.mlmodel` would allow running it on the Apple Neural Engine or GPU for efficiency. The model is small enough that even CPU inference might suffice, especially if using the 16-bit quantized TFLite version (which the authors report works on a Raspberry Pi with ~70% of a core).
- **Performance**: Real-time capable. The original model processes a 32 ms audio frame in ~0.65 ms on a midrange Intel CPU. That's ~50× faster than real-time on a laptop CPU. In practice on an iPhone CPU core, you might get a few milliseconds per frame – well within 1× real-time (and using Core ML on the Neural Engine could further reduce CPU usage). In a practical test, the 16 kHz quantized model ran in real-time on a Raspberry Pi 3B+ (borderline but worked), so any modern iPhone can handle it comfortably. Latency is typically one frame (20–32 ms) plus minimal processing overhead.
- **Core ML/Metal**: Supported via conversion. `DTLN`'s ONNX model can be converted to Core ML. It consists mainly of LSTM layers and dense layers, which Core ML can execute (Core ML 3+ supports LSTMs). No custom Metal shaders are needed beyond what Core ML provides. Alternatively, one can run the TFLite model using TensorFlow Lite library on iOS (CPU or via Core ML delegate).
- **License**: MIT/Apache 2.0 (the reference implementation and model are open-source).

**Pros**: Modern neural model with good noise suppression and some dereverb capability. Small model size (~4 MB) and fast inference. Open-source – can be modified or retrained for specific noise types. Already available in formats friendly to mobile (TFLite/ONNX).
**Cons**: Integration is more involved (requires ML framework). You must manage the model inference pipeline (frame input, overlap-add of audio, etc.). Slightly higher runtime cost than `RNNoise` (though still within real-time margin).

### DeepFilterNet (and DeepFilterNet2)

`DeepFilterNet` is a recent open-source project focusing on low-complexity speech enhancement using a "deep filtering" technique. It uses a two-stage approach: first enhancing the coarse spectral envelope, then using a learned filter to clean the fine structure of the speech (particularly the periodic components like pitch harmonics). The authors specifically target embedded devices and full-band audio (48 kHz). `DeepFilterNet2` (2022) further improves quality and efficiency, reportedly achieving state-of-the-art enhancement quality while keeping real-time factor very low.
- **Quality**: `DeepFilterNet2` is among the state-of-the-art in non-real-time benchmarks (e.g., winning or top-ranking in DNS Challenge for some tracks). It handles both noise and reverberation well, due to its two-stage design and use of complex spectral features. In the authors' evaluations, `DeepFilterNet2` got very close to the best intelligibility and noise reduction scores, while being significantly faster. It explicitly aims to preserve speech quality (high STOI and MOS) while removing noise, avoiding artifacts common in aggressive noise suppression.
- **Integration**: High effort. The project is written in Rust (with some Python for training). It provides pretrained models and even a ready C/C++ binary for offline processing, but it's not packaged for mobile. To use it on iOS, one could compile the Rust library to a static lib or use the provided ONNX (if available – the authors mention PyTorch models). There is a LADSPA plugin for real-time use on Linux, showing it can run in realtime. However, adapting it to iOS (Core ML or TFLite) would require converting the model and reimplementing its inference (possibly complex due to custom filtering operations).
- **Performance**: Likely real-time, but pushing limits on CPU. The authors note `DeepFilterNet2` runs in real-time on a Raspberry Pi 4. On an i5 notebook CPU, the real-time factor is reported very low (indicating fast performance). However, it's a larger model than `RNNoise`/`DTLN`. It might need Neon/SIMD optimizations or partial GPU offload to comfortably run on mobile at 48 kHz. Since it processes full-band audio, downsampling to 16 kHz could reduce load if wideband is not needed.
- **Core ML/Metal**: Partially possible. The core neural network could be converted to Core ML, but the algorithm also involves a custom filtering mechanism. It may require writing some custom Metal shader or doing part of processing in custom code. This is not a plug-and-play Core ML model integration.
- **License**: MIT/Apache 2.0 (dual) – fully open-source.

**Pros**: Top-tier noise and echo removal, with academic backing. Could future-proof quality-wise (as it's cutting edge in 2025). Open-source and customizable.
**Cons**: Very involved to integrate on iOS currently. Higher runtime cost – might need to leverage the Neural Engine or be restricted to offline (non-real-time) use on device until optimized further. If time-to-deploy is a concern, this is a longer-term option.

*(If pursuing `DeepFilterNet`, one might start by using their precompiled CLI on a desktop to evaluate quality on your audio, then decide if it's worth porting. Alternatively, keep an eye on mobile-focused forks or lighter variants.)*

### Other Notable Models
- **NSNet 2 (Microsoft)** – a successor to `RNNoise`-style models, used as a baseline in Microsoft's DNS Challenge. Microsoft has published models that can run real-time and address noise + reverberation. However, there isn't an official iOS SDK. You could find the model in the open DNS-Challenge repo and convert it to Core ML. Effort is similar to `DTLN`. Quality is good, but community support for `DTLN` is better due to the open implementation.
- **Speech-BERT or MetricGAN** – more advanced GAN or transformer models for speech enhancement exist, but they are far too heavy for mobile real-time (often tens of millions of parameters). Those are better suited for cloud processing.
- **"Post-filter" algorithms** – e.g., using a small ML model to post-process audio after a traditional filter. If ultra-low latency is needed, one could use a simple noise suppressor (like WebRTC's) and then a tiny model to further clean speech. This however gets complex and usually the end-to-end models like above perform better.
- **Mask-Based Enhancement (General Technique):** Many deep learning approaches utilize predicted time-frequency masks (e.g., Ideal Ratio Mask - IRM, Complex Ratio Mask - CRM) applied to the input spectrogram to suppress noise or isolate speech. Models like U-Net are often trained to predict these masks. While effective, the term "MaskSR" itself doesn't point to one specific model but rather this common technique category.
- **SpeechX:** A versatile speech generation model ([Paper](https://paperswithcode.com/paper/speechx-neural-codec-language-model-as-a), Demo: [aka.ms/speechx](https://aka.ms/speechx)) based on neural codec language modeling and multi-task learning (Aug 2023). It aims to perform zero-shot TTS, noise suppression, target speaker extraction, and other speech transformation tasks using task-specific prompts, handling both clean and noisy audio. An official open-source implementation isn't readily available. Its complexity might make on-device real-time use challenging without significant optimization.
- **SpeechFlow / Supervoice Flow:** Based on the "Generative Pre-training for Speech with Flow Matching" paper, this approach uses flow matching models as a foundation for various speech tasks. The `ex3ndr/supervoice-flow` implementation ([GitHub](https://github.com/ex3ndr/supervoice-flow), MIT License) can reportedly be fine-tuned for speech enhancement (e.g., "Supervoice Enhance" for noise cleanup), TTS, and diarization. Flow matching models are generative and potentially powerful but can be computationally intensive. Their suitability for real-time, on-device enhancement depends on model size and optimization. (Note: The `just-ai/speechflow` toolkit focuses primarily on TTS using related techniques.)
- **NVIDIA NeMo:** A comprehensive NVIDIA framework ([GitHub](https://github.com/NVIDIA/NeMo), Apache-2.0 License) for large-scale development of conversational AI models (ASR, TTS, NLP, etc.). While it contains advanced speech processing components, it doesn't feature dedicated speech enhancement models as a primary offering within the open-source toolkit itself. Enhancement functionalities might be available through the related NVIDIA Riva deployment platform or could potentially be built by adapting NeMo components, but it's not an out-of-the-box feature. Likely too complex for direct on-device enhancement use unless specific optimized components are extracted.
- **UniAudio:** A foundation model framework ([GitHub](https://github.com/yangdongchao/UniAudio), MIT License, Demo: [Link](http://dongchaoyang.top/UniAudio_demo/)) designed for universal audio generation using neural codecs and language modeling ([arXiv:2310.00704](https://arxiv.org/pdf/2310.00704.pdf)). It explicitly lists speech enhancement, dereverberation, and speech extraction among its many supported tasks. It uses a unified approach where tasks are defined, data is tokenized, and a single large model is trained/used. Pre-trained checkpoints are available. Given its foundation model nature, it might be computationally intensive for real-time on-device use without optimization.
- **AudioSR:** A diffusion-based model specifically for versatile Audio Super-Resolution ([GitHub](https://github.com/haoheliu/versatile_audio_super_resolution), MIT License). It upsamples audio from various input bandwidths (e.g., 2-16 kHz) to 48 kHz sampling rate (24 kHz bandwidth), restoring high-frequency components for speech, music, and sound effects. It performs best when input degradation resembles a clean low-pass filter. As a diffusion model, real-time on-device performance might be challenging.

## Traditional DSP-Based Solutions

If a machine learning approach is too complex to integrate initially, there are classic noise reduction algorithms that can be used on iOS with relatively little fuss. These generally won't match the speech clarity improvement of modern ML (especially in very noisy cases), but they can still be valuable for moderate noise and have low resource usage.

### WebRTC Audio Processing (Noise Suppression)

Google's WebRTC project includes an Audio Processing Module (`APM`) widely used in VoIP apps. It provides: Noise Suppression, Acoustic Echo Cancellation, Automatic Gain Control, and Voice Activity Detection. The noise suppression in WebRTC `APM` is a traditional spectral noise suppressor that continuously estimates background noise and subtracts it. It's optimized in C/C++ for real-time operation on mobile devices.
- **Quality**: The WebRTC NS is decent for steady background noises (air conditioning, office noise). It tends to introduce minimal artifacts at its default setting. It may struggle with non-stationary noises (e.g. keyboard clicks) and doesn't do much for reverberation. It also won't boost speech beyond noise removal (no learning of speech characteristics, unlike ML). Quality is generally inferior to `RNNoise` in heavy noise scenarios – in fact, `RNNoise` was demonstrated to outperform the classic suppressor for 20dB car noise, for example. Still, it's a solid baseline and avoids the "weird" artifacts sometimes heard with mis-tuned neural nets.
- **Integration**: Medium effort. You can either compile the WebRTC `APM` as a library (there are third-party wrappers, e.g. `webrtc_audio_processing` library) or use an existing wrapper. For instance, the `SpeexDSP` library also has a similar noise suppressor (`Speex`'s NS was a basis for older WebRTC NS) – `SpeexDSP` is easy to compile in C. There's also a Python wrapper for WebRTC `APM` (for reference) which calls its NS and AGC on 10 ms frames. On iOS, you could integrate the C code and call `ProcessStream` on each 10 ms or 20 ms frame of PCM audio. WebRTC `APM` expects 16 kHz mono audio for its processing chain (and can work at 48 kHz in newer versions).
- **Performance**: Real-time (very lightweight). This module was designed for mobile from day one. CPU usage is low – in WebRTC usage on iPhones, it runs comfortably within the audio thread's time budget. It processes audio in blocks (typically 10 ms frames) and the Real-Time Factor is far below 1. In practical terms, you might see <5% CPU usage on a single core for NS + AEC. The memory footprint is tiny (just a few kilobytes for internal buffers).
- **Core ML/Metal**: N/A. This is pure C/C++ DSP, no ML or GPU.
- **License**: BSD-style (WebRTC is BSD licensed). `SpeexDSP` is BSD-compatible (Xiph.org license).

**Pros**: Very stable and battle-tested (used in Chrome, Firefox, WhatsApp, etc.). Straight C/C++ with no external deps. Runs in constant time. Also, you get AEC and AGC if needed for future live use.
**Cons**: Not as effective as learning-based methods in challenging audio. It won't improve speech clarity except by removing noise – e.g., it won't remove mild reverb or mic muffling. Some tuning might be needed to avoid over-suppression (where it might accidentally attenuate speech if mis-configured).

*(Note: If you use WebRTC NS, set the suppression level to "High" for maximal noise removal, or "Very High" if available, at the cost of slight speech distortion. The APM also has an experimental dereverb component, but it's not very impactful.)*

### Custom FFT Noise Reduction or Filters

One can implement a basic noise reduction using the Accelerate `vDSP` framework on iOS to perform FFTs. For example:
- Capture a noise profile from a segment of "silence" then subtract that spectrum from speech (Spectral Subtraction).
- Apply a multi-band noise gate or Wiener filter that attenuates frequencies where noise is dominant.
- Use dynamic range compression or EQ to boost speech formant regions (~1–4 kHz) for clarity.

These approaches require signal processing expertise and tuning. They are less adaptive than ML – for instance, a spectral subtractor might struggle if noise characteristics change over time. Nonetheless, for certain consistent noise (like static hum), a manual filter can help.

**Pros**: Full control, no external dependencies. Can run on CPU very fast using `vDSP` FFT and vector ops.
**Cons**: Reinventing the wheel – quality likely lower than ready solutions. Development time to tweak algorithms could be significant. Given the availability of `RNNoise` and others, custom DSP is usually only worth it for simple use-cases or as a supplement to another method.

## Summary of Options

Below is a comparison of the key solutions discussed:

| Solution               | Type                 | Integration Effort                 | Performance                 | Core ML/Metal                     | License                       |
|------------------------|----------------------|------------------------------------|-----------------------------|-----------------------------------|-------------------------------|
| Krisp SDK              | ML SDK (Proprietary) | Very Easy (drop-in SDK)            | ✓ Real-time (optimized)     | No (closed, internal opts)      | Commercial (contact Krisp)    |
| Picovoice Koala        | ML SDK (Hybrid RNN)  | Very Easy (SPM/Pod)              | ✓ Real-time (lightweight)   | No (custom engine, CPU)         | Apache-2.0 (needs key)        |
| Apple VoiceProcessing  | DSP (built-in)       | Easy (live), tricky (file)         | ✓ Real-time (hardware)      | No (built into iOS)             | Included in iOS (closed)      |
| RNNoise                | ML (small RNN)       | Medium (C library)                 | ✓ Real-time (60× on PC)     | Not needed (CPU sufficient)     | BSD-3-Clause                  |
| DTLN                   | ML (LSTM)            | Medium (Core ML/TFLite)          | ✓ Real-time (50× on PC)     | Yes (convert ONNX to CoreML)    | MIT (open source)             |
| DeepFilterNet2         | ML (DF + DNN)        | High (custom integration)          | ~Real-time (high-end device)| Partial (custom logic needed)   | MIT/Apache-2.0                |
| WebRTC NS (APM)        | DSP (spectral)       | Medium (C++ library)               | ✓ Real-time (low CPU)       | No                                | BSD-style (WebRTC)            |
| SpeexDSP NS            | DSP (spectral)       | Medium (C library)                 | ✓ Real-time (low CPU)       | No                                | BSD (Xiph.org)                |


## Recommendations

For a quick win on local MP4 processing, a ready-made SDK is attractive. If licensing and cost are not barriers, `Krisp SDK` offers top-tier noise removal with minimal dev work. If you prefer an easier-to-access solution with an open license, Picovoice `Koala` is a strong choice – it's free for trial, easy to integrate, and efficient, while still significantly enhancing speech intelligibility.

If you lean toward open-source and long-term flexibility, start with `RNNoise` (easy to get working and see an immediate improvement in SNR). For better results, plan to integrate an advanced model like `DTLN` via Core ML – it strikes a good balance of quality vs. efficiency. `DTLN` can likely handle mild reverberation and tough noise better than `RNNoise`. As you move to real-time streaming (WebRTC), these models can be used frame-by-frame, or you might combine them with the WebRTC APM (for example, use AEC from WebRTC and then an ML model for noise).

**Lightweight real-time considerations**: `RNNoise` and `Koala` are proven on mobile for continuous real-time use. `DTLN` should also run in realtime on newer iPhones (especially if using the Neural Engine for inference). `DeepFilterNet2` might be borderline for real-time on current iPhones without optimization – it could be something to watch as devices get more powerful or if the model is pruned further.

Finally, remember that no single method is perfect. In practice, you might use a combination: e.g., a primary denoiser (ML model) and a secondary filter (e.g., a high-pass to remove low rumble, or a dereverb algorithm for residual echo). Start with one of the above solutions and evaluate on your audio. Since everything can run on-device, you can iterate quickly. By beginning with a solid baseline (`Koala` or `RNNoise`) and then experimenting with more advanced models, you'll equip your iOS app with robust speech enhancement ready for both offline files and future real-time streams.

## VoiceFixer for On-Device Speech Enhancement: Feasibility Analysis

### VoiceFixer Model Architecture and Complexity

`VoiceFixer` is a two-stage deep learning model for speech restoration. It consists of an analysis stage that uses a deep residual U-Net (`ResUNet`) and a synthesis stage that uses a neural vocoder. In the analysis stage, the `ResUNet` operates on an input spectrogram (e.g. a mel-spectrogram derived from the degraded audio) to perform multi-task enhancement: denoising, dereverberation, bandwidth expansion (super-resolution), and declipping all within one model. The output of the `ResUNet` is an enhanced mel-spectrogram, which the vocoder then converts back to a waveform (similar to how speech synthesis systems generate audio). This design mimics human hearing: first analyze the content of speech, then synthesize a clean speech signal.

The model is quite large and complex. The authors explored smaller vs larger U-Net configurations (e.g. a "UNet-S" vs full UNet) during research. The full `VoiceFixer` model uses a substantial `ResUNet` (multiple convolutional blocks) and a high-fidelity vocoder. In total, `VoiceFixer` likely has tens of millions of parameters (the exact count isn't stated in the README, but related models for speech enhancement can exceed 50–100 million). For example, one similar speech super-resolution model (`NVSR`) had ~99 million parameters, with ~34M in the vocoder alone. `VoiceFixer`'s vocoder is a "universal" 44.1 kHz model (speaker-independent) provided with the project. This vocoder is likely based on a GAN architecture (the paper references HiFi-GAN and similar neural vocoders) to achieve high-quality audio. Such vocoders typically involve multiple layers of convolution and upsampling to generate 44.1 kHz audio.

**Expected size and resources**: The `VoiceFixer` PyPI package will download hefty model weight files when first run (the code is small, but weights are large). During training, the authors needed 4× NVIDIA V100 GPUs (32GB each) for two days to train `VoiceFixer` on a dataset of 500+ hours – indicating the model's complexity. In practice, the model will also be memory-intensive at runtime due to the high sampling rate (44.1 kHz) and the need to handle spectrograms with large dimension (e.g. 128 mel bands, large FFT, etc.). This complexity suggests that running `VoiceFixer` on-device will be challenging, especially for real-time use, without significant optimization.

### Inference Performance and Real-Time Feasibility

`VoiceFixer` was primarily designed for offline speech restoration quality, not optimized for low-latency streaming. In offline evaluations it produced significantly higher speech quality (MOS) than simpler enhancement models, but no mention of real-time operation is made in the paper or docs. Given its architecture, we should temper expectations on speed:
- **CPU Performance**: On a typical CPU, running the full `VoiceFixer` pipeline (`ResUNet` + vocoder) is likely slower than real-time. The neural vocoder is the most compute-intensive part – it must generate high-fidelity audio. Even efficient vocoders like HiFi-GAN can be heavy: the highest-quality HiFi-GAN model runs much faster than older WaveNet but still may struggle on mobile CPUs unless reduced in size. The HiFi-GAN paper introduced a "small footprint" vocoder (~0.9M params) that can run ~13× faster than real-time on a CPU. However, `VoiceFixer`'s vocoder (for universal 44.1 kHz) is likely larger than that tiny version, trading speed for quality. A larger vocoder may not achieve real-time on a mobile CPU. In addition, the `ResUNet` analysis network adds overhead for computing the enhanced spectrogram.
- **GPU/Accelerator Performance**: For near real-time needs, hardware acceleration would be essential. On a desktop GPU, `VoiceFixer` can likely run faster than real-time (the authors' demo in a Colab notebook records 6 seconds of audio and restores it, implying it can process a few seconds of audio relatively quickly) – but mobile GPUs or NPUs have different performance profiles. An Apple A-series or M-series chip has a powerful Neural Engine (ANE) and GPU which could accelerate a converted model. For instance, others have shown heavy models like OpenAI's Whisper ASR run much faster with Core ML on the Neural Engine. We can expect that with Core ML/Metal performance optimizations, an iPhone's Neural Engine could substantially speed up inference. Without such acceleration, CPU-only operation on iOS might process significantly slower than 1× real-time (possibly only a few seconds of audio per minute of processing, though exact figures would depend on model optimizations and device generation).
- **Real-Time Streaming (WebRTC) Feasibility**: Out-of-the-box, `VoiceFixer` processes whole audio clips (wav files) and is not designed for frame-by-frame streaming. To use it in live audio (WebRTC), one would need to chunk the audio into frames, run the model continuously, and stitch outputs. This is non-trivial because the vocoder works on a sequence of mel spectrogram frames; very short segments might cause artifacts or require overlap-handling. Achieving low latency (<~100 ms) with such a large model is unlikely on current mobile hardware without major modifications. In Stage 2 (live calls), `VoiceFixer` in its current form is probably not practical for real-time – it would need downsizing or a streaming-friendly redesign. By contrast, simpler noise suppression models often operate on 20ms frames or smaller for real-time; `VoiceFixer`'s heavy analysis/synthesis per large window would introduce much more delay.

In summary, `VoiceFixer` is best suited for offline processing (Stage 1: enhancing audio files) where it can take a few seconds or more to process each clip. For near real-time use, significant optimization or using a smaller model is likely required. If the goal is eventually live enhancement, one might use `VoiceFixer` as an offline "quality benchmark" and deploy a lighter model for Stage 2.

### Porting VoiceFixer to iOS (Core ML / TFLite Conversion)

Porting `VoiceFixer` to run on iOS will require model conversion and possibly splitting the pipeline:
- **Framework Conversion**: `VoiceFixer`'s implementation is in PyTorch. To run on iOS, you would typically convert the model to Core ML (for use with Apple's ML framework) or to TensorFlow Lite (and use TFLite C++ on iOS). Conversion is possible but not one-click. You'd need to export the PyTorch models (`ResUNet` and vocoder) to an interchange format like ONNX, then convert to Core ML using `CoreMLTools` or to TFLite via a TensorFlow reimplementation. Each stage (spectrogram enhancer and vocoder) could be converted separately. Core ML supports convnets and even transposed convolutions (used in vocoders) fairly well, so the operations should be convertible. Care must be taken with any custom operations (e.g. STFT or istft). In `VoiceFixer`'s case, the STFT/Mel extraction is likely done in preprocessing Python code, not as part of the neural net graph, which is actually helpful – it means the neural network itself might just take a mel-spectrogram tensor as input and output a waveform tensor.
- **Pipeline on iOS**: You may need to rebuild parts of the pipeline in Swift. For example:
  1. Decode the MP4 file audio to PCM (`AVFoundation` can do this) and convert it to a numpy array or `MLMultiArray`.
  2. Compute the mel-spectrogram of the audio. This could be done on device using Accelerate FFT or via a tiny Core ML model for STFT, or even by feeding the raw waveform into the `ResUNet` if that model is trained to include an internal STFT layer. (`VoiceFixer` likely expects a mel input, so doing the STFT/mel in Swift is simplest.)
  3. Run the `ResUNet` model (converted to Core ML) on the mel spectrogram to get an enhanced mel spectrogram.
  4. Run the vocoder model (converted to Core ML) on the enhanced mel to generate the cleaned waveform.
  5. Play or save the output waveform.
Converting both the analysis and synthesis models to Core ML yields potentially two Core ML models. The good news is that `VoiceFixer`'s code is already modular (the `VoiceFixer()` class in Python likely loads two sets of weights). You could also fuse them into one model that takes waveform input and outputs waveform, but that would require embedding the FFT and feature extraction inside the model graph, which complicates conversion.
- **Conversion Difficulty**: Expect some challenges. ONNX conversion of complex PyTorch models can hit unsupported operations, especially with dynamic shapes or custom layers. The neural vocoder might use advanced ops (e.g. upsampling, dilated convs) – these are generally supported in Core ML, but ensuring the ONNX exporter captures everything is crucial. If ONNX fails, an alternative is to reimplement the network in TensorFlow (perhaps using an equivalent Keras model for U-Net and vocoder) and use the provided weights, then use TF Lite or Core ML. `VoiceFixer` doesn't have an official TF/TFLite version, so this would be a custom effort.
- **Model Size & Optimization**: The full FP32 model will be large (potentially hundreds of MB of weights). This is not ideal for mobile apps. You would want to quantize or compress the model. Core ML supports 16-bit weights or even 8-bit quantization. Quantizing the models (especially the vocoder) might degrade quality slightly, but could be necessary to fit within memory and run efficiently. Apple's tools and guidelines offer ways to compress models and reduce memory and power usage. Additionally, you might split the computation across CPU/GPU/ANE. For instance, the `ResUNet` could run on the Neural Engine while the vocoder runs on the GPU, or vice-versa, to maximize throughput. TFLite also has a GPU delegate and NNAPI (on Android) – on iOS, TFLite can use Metal via Core ML delegate as well.

In short, converting `VoiceFixer` to an on-device model is feasible (there are no fundamental theoretical blockers), but it will require careful engineering. This includes exporting the model, integrating pre/post-processing code in the app, and optimizing speed. There are no known publicly documented cases of `VoiceFixer` running on iOS yet, so you would be charting new territory. It's a significantly heavier lift compared to using a ready-made mobile-oriented model.

### CPU-Only vs Hardware Acceleration Requirements

Given the complexity, CPU-only operation would be marginal for any real-time or even near-real-time use:
- **For offline file processing (Stage 1)**, CPU-only might be acceptable if the user can wait. For example, enhancing a 5-minute audio clip might take on the order of minutes on an iPhone CPU. This could be fine for a background task (as long as it doesn't drain too much battery), but it's not instantaneous.
- **For live audio (Stage 2)**, CPU-only is likely impractical – it would lag far behind the audio stream. To hit real-time, you would need to leverage the device's Neural Engine (ANE) or GPU. These accelerators can perform the parallel convolutions much faster. As noted, Apple's ANE dramatically speeds up ML inference for well-optimized Core ML models (e.g., giving multi-fold speedups for models like Whisper).

**Neural Engine vs GPU**: Core ML will automatically decide whether to run a model on CPU, GPU, or ANE based on the model's layers and sizes (you can also force a choice). The ANE is very efficient for CNNs and could likely handle `VoiceFixer`'s convolutional layers in real-time if the model fits in its memory. The GPU (with Metal Performance Shaders) can also accelerate the conv layers, though possibly at higher power cost than ANE. The CPU might handle some scalar tasks (like any remaining processing or smaller layers). It is almost certain that hardware acceleration is needed for Stage 2 – a pure ARM CPU implementation would risk audio dropouts or high latency.

Another consideration is multi-threading: On CPU, one could use multi-core processing (e.g., using Accelerate/`vDSP` for FFT and multi-threaded convs). But writing a custom multi-threaded inference is a lot of work; it's easier to rely on Core ML's utilization of ANE/GPU.

**Memory and battery**: Running a large model in real-time will consume significant battery. Offloading to ANE is beneficial because it's optimized for low-power execution of neural nets. Using the GPU heavily for ML can impact graphics performance and battery. So from a practical deployment standpoint, to make `VoiceFixer` viable on iOS, one should aim to use Core ML on ANE (A12 Bionic and above) for the heavy lifting.

### Comparison with Other On-Device Solutions

When considering on-device speech enhancement, `VoiceFixer` vs other solutions involves trade-offs in capability, performance, and integration effort:
- **RNNoise (Mozilla/Xiph)** – Noise suppression via RNN: `RNNoise` is a classic lightweight denoising model (combines signal processing with a small GRU neural net). It's extremely small (~85 kB model) and fast, designed for real-time on CPU. It can run on tiny devices (even a Raspberry Pi) with minimal latency. Integration is straightforward: it's implemented in C and can be compiled into an iOS app (e.g., via WebRTC's noise suppression or as a standalone). **Advantages**: Very low CPU usage, no special hardware needed; open-source (BSD/MIT-like license). **Drawbacks**: It only addresses background noise, not other issues. Its effect can sound robotic or under-water if noise is severe, and it won't improve reverberation, clipping, or bandwidth – so the resulting quality improvement is limited compared to `VoiceFixer`. `RNNoise` works well for mild to moderate noise in speech and is battle-tested for live calls, but it won't perform miracles on badly degraded audio.
- **DTLN (Dual-Transfer Learning Network)** – Modern real-time denoiser: `DTLN` is a 2020 deep learning model focusing on real-time noise suppression with low latency. It uses a two-stage approach with STFT and learned features, built on LSTM layers, and has <1 million parameters. It was designed to run streaming with frame-in frame-out processing. In the Deep Noise Suppression challenge, `DTLN` achieved good noise reduction and ran in real-time. **Advantages**: It offers better noise suppression quality than `RNNoise` (leveraging a deeper neural network) and also modest model size. Importantly, pre-trained models and converters to TFLite and ONNX are available. This makes integration easier – you could directly use a TensorFlow Lite model on iOS. It's also MIT licensed (open source). On a Raspberry Pi 4, the quantized `DTLN` model runs in under 10ms per 8ms frame, and on an iPhone CPU it should likewise achieve real-time or close (likely utilizing about 20-30% of a high-performance core for 16 kHz audio). **Drawbacks**: Like `RNNoise`, `DTLN` is primarily for noise removal. It does not inherently do super-resolution or severe dereverberation (though it may implicitly reduce mild reverb as part of noise). It's focused on making speech cleaner in background noise scenarios (e.g. driving noise, cafe noise). If the MP4 files or streams need bandwidth expansion (e.g. telephone bandwidth to fullband) or clipping repair, `DTLN` won't handle that – whereas `VoiceFixer` would (`VoiceFixer` can upsample 8 kHz audio to 44.1 kHz and fill in missing frequencies). Training or fine-tuning would be needed to extend `DTLN` to other tasks. However, for the common use-case of "remove background noise in real-time," `DTLN` is a strong candidate with much lower integration effort than `VoiceFixer`.
- **Picovoice Koala** – Proprietary on-device noise suppression: `Koala` is a commercial SDK specifically engineered for on-device speech enhancement. It targets noise cancellation in a streaming context and is optimized for platforms including iOS. **Advantages**: `Koala` is designed for developers who want a plug-and-play solution. It runs locally with low latency (designed to keep latency well under 200ms), and the quality is reported to be higher than `RNNoise` (Picovoice claims 4-5× better noise reduction in their benchmarks). Integration on iOS is straightforward – Picovoice provides an iOS SDK (Swift/C interface) so you can feed it audio buffers and get back enhanced audio in real-time. It is cross-platform and hardware-optimized (likely using ARM Neon or even Core ML under the hood). **Drawbacks**: It's not open-source – `Koala` is proprietary and requires a license. The company offers a limited free tier (e.g. 25 hours/month of processing under a trial license) and then paid plans. This means for a consumer app, you'd have to manage license keys and possibly cost. Also, `Koala`'s focus is noise suppression; it's not intended for other distortions (no upscaling of sample rate, etc.). If your use-case is mainly to suppress background noise in calls or recordings, `Koala` can be an excellent solution with minimal development effort. But if you need the broader "speech restoration" (fixing heavy distortion, increasing fidelity of old recordings), `Koala` would not cover that – that remains `VoiceFixer`'s unique strength.

In summary, compared to `VoiceFixer`, these alternatives sacrifice breadth of enhancement for efficiency. `VoiceFixer` can address a wider range of problems (one model for noise, reverb, clipping, and super-resolution), producing remarkably improved audio quality on very degraded inputs. But it is computationally heavy and currently exists as a research codebase. The alternatives (`RNNoise`, `DTLN`, `Koala`) focus mostly on noise removal:
- They are lightweight and real-time (designed for streaming from the start).
- They have ready-to-use implementations on mobile (or at least easily convertible, in the case of `DTLN`).
- However, they won't, for example, magically restore a muffled 4 kHz audio to crisp wideband, whereas `VoiceFixer`'s neural vocoder can add detail to low-quality audio.

A practical approach might be hybrid: use `VoiceFixer` offline for heavy restoration when time permits (Stage 1), and use a light real-time suppressor (like `RNNoise`/`DTLN`/`Koala`) for live calls (Stage 2). This way, users get some enhancement live, and can post-process recordings with a stronger model if needed.

### License and Integration Considerations

**VoiceFixer License**: The `VoiceFixer` project is released under the MIT License, which is very permissive for commercial and private use. This means you can modify it, integrate it into an app, and even redistribute it as part of your app without concern, as long as you include the license notice. The pretrained model weights provided are also under MIT (there is no non-commercial restriction noted). This is good news for an iOS app – you won't have legal barriers using `VoiceFixer` code or models on-device.

For the alternatives:
- `RNNoise` is also permissively licensed (it comes from Xiph/Mozilla, under a BSD-type license). It's free to use and integrate. In fact, it's already integrated in some open-source projects (e.g., OBS Studio, WebRTC's noise suppression module).
- `DTLN` is open-source under MIT as well. You can use the code and even fine-tune the model if needed. The availability of pretrained TFLite and ONNX models simplifies integration – you might not need to retrain anything.
- `Koala` by Picovoice is closed-source and provided as a binary SDK. Its license terms require using their API with a key. As mentioned, it enforces usage limits unless you pay for higher tiers. So, while integration technically is easy, you are essentially locked into Picovoice's service model. If your project is commercial, you'd factor in those costs and the requirement that the app can reach the license server (for key validation).

**Integration Effort Summary**: `VoiceFixer` will demand significant engineering investment to get running on iOS (model conversion, performance tuning, custom pipeline code). `RNNoise` and `DTLN` can be integrated much more quickly – `RNNoise` by compiling a C library, `DTLN` by loading a ready TFLite model. `Koala` is the quickest (just use the provided framework and API), but you trade off freedom and possibly cost. If your ultimate goal is to have an on-device, real-time enhancement in an iOS app, you might lean toward `DTLN` or `Koala` for Stage 2 to minimize development time and ensure smooth performance. On the other hand, for offline processing of files (Stage 1) where quality is the priority, `VoiceFixer` could be worth the effort if its multi-faceted restoration yields audibly better results on your target audio than simpler denoisers. It may also be possible to use `VoiceFixer` on a server or desktop to process files, if on-device proves too slow, while using a lighter model on-device for live audio.

## References and Further Reading
- **VoiceFixer Paper** (Liu et al. 2021) – "Toward General Speech Restoration with Neural Vocoder." Describes the two-stage architecture (`ResUNet` + vocoder) and demonstrates superior quality on noisy, reverberant, band-limited speech.
- **VoiceFixer GitHub README** – Contains usage examples and notes that `VoiceFixer` handles noise, reverberation, low-resolution (2 kHz~44.1 kHz) and clipping in one model. The project is MIT licensed and provides a pip package for easy deployment.
- **HiFi-GAN Vocoder** (Kong et al. 2020) – Demonstrates a fast GAN-based vocoder. A tiny version can run 13× faster than real-time on CPU, suggesting that vocoders can be optimized for speed at some cost to quality. `VoiceFixer` likely uses a similar vocoder approach for 44.1 kHz audio.
- **RNNoise** (Valin 2017) – Blog and demo of `RNNoise`, a classic low-footprint noise suppressor that runs in real-time on a Raspberry Pi (model fits in 85 kB after quantization). Illustrates the opposite end of the spectrum (tiny model, fast, but limited enhancement capability).
- **DTLN** (Braun & Tashev 2020) – Interspeech 2020 paper introducing `DTLN`. The open-source implementation notes <1 million params and real-time performance on devices, with ready-made TFLite models for deployment. Good resource for a balanced noise reduction network that is mobile-friendly.
- **Picovoice Koala Announcement** – Picovoice's article claiming `Koala`'s noise suppression outperforms `RNNoise` by 4-5× and runs across platforms including iOS with low latency. Useful to understand commercial options and their trade-offs (quality vs openness vs cost).

## Additional opportunities

AnyEnhance - https://arxiv.org/html/2501.15417v1
CleanMel - https://arxiv.org/html/2502.20040v1
UnDiff - https://arxiv.org/abs/2306.00721
SGMSE - https://arxiv.org/abs/2208.05830